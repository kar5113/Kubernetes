#### Installing kubernetes with aws eksctl
  1. Install kubectl command line tool
    For linux:
    ```bash
    curl -O https://s3.us-west-2.amazonaws.com/amazon-eks/1.34.2/2025-11-13/bin/linux/amd64/kubectl
    chmod +x ./kubectl
    mkdir -p $HOME/bin && cp ./kubectl $HOME/bin/kubectl && export PATH=$HOME/bin:$PATH 

  2. Install eksctl command line tool
    For linux:
    ```bash
    # for ARM systems, set ARCH to: `arm64`, `armv6` or `armv7`
    ARCH=amd64
    PLATFORM=$(uname -s)_$ARCH
    curl -sLO "https://github.com/eksctl-io/eksctl/releases/latest/download/eksctl_$PLATFORM.tar.gz"
    # (Optional) Verify checksum
    curl -sL "https://github.com/eksctl-io/eksctl/releases/latest/download/eksctl_checksums.txt" | grep $PLATFORM | sha256sum --check
    tar -xzf eksctl_$PLATFORM.tar.gz -C /tmp && rm eksctl_$PLATFORM.tar.gz
    sudo install -m 0755 /tmp/eksctl /usr/local/bin && rm /tmp/eksctl 

  3. Create a kubernetes cluster with eksctl
    ```bash
    eksctl create cluster --name=<cluster-name> --region=<aws-region> --nodes=<number-of-nodes> --node-type=<instance-type>
    # Example:
    eksctl create cluster --config-file=cluster-config.yaml
    eksctl delete cluster --config-file=cluster-config.yaml


  4 . to access the cluster, aws cli should be configured with proper access and secret keys
    ```bash
    aws configure
    # Provide AWS Access Key ID, AWS Secret Access Key, Default region name, Default output format
    aws eks update-kubeconfig --region <aws-region> --name <cluster-name>  



#### kubernetes commands
- kubectl api-resources
- kubectl get nodes    
- kubectl get pods
- kubectl get services
- kubectl get deployments
- kubectl describe pod <pod-name>
- kubectl describe service <service-name>
- kubectl describe deployment <deployment-name>
- kubectl logs <pod-name>
- kubectl apply -f <file.yaml>
- kubectl delete -f <file.yaml>
- kubectl exec -it <pod-name> -- /bin/bash
- kubectl scale deployment <deployment-name> --replicas=<number-of-replicas>
- kubectl port-forward <pod-name> <local-port>:<pod-port>
- kubectl get namespaces
- kubectl create namespace <namespace-name>
- kubectl delete namespace <namespace-name>
- kubectl config use-context <context-name>
- kubectl get all -n <namespace-name>
- kubectl exec -it <pod-name> -c <container-name> -- <command> # Execute command in specific container within a pod
- kubectl exec -it <pod-name> -- <command> # Execute command in the first container of the pod if multiple containers exist or single container pod
- kubectl config set-context --current --namespace=roboshop-dev # Set default namespace for current context or use kubens...
- kubectx <context-name> # Switch between contexts easily using kubectx tool when multiple clusters are used
- kubens <namespace-name> # Switch between namespaces easily using kubens tool when multiple namespaces are used
- kubectl get nodes --show-labels # Show labels of nodes for getting selectors.
- kubectl taint nodes <node-name> key=value:effect # Add taint to a node so that only pods with matching tolerations can be scheduled on it, others will be repelled
- kubectl taint nodes node1 key1=value1:NoSchedule- # to remove the taint from the node
- kubectl taint nodes <node-name> key1=value1:effect- # to remove specific taint from the node

- aws eks update-kubeconfig --region <region> --name <cluster-name> # to update kubeconfig for eks cluster


#### kubernetes resource types
- Pods
- Services
- Deployments
- ReplicaSets
- Namespaces
- ConfigMaps
- Secrets
- PersistentVolumes
- PersistentVolumeClaims
- StatefulSets
- DaemonSets
- Ingress   
- Jobs
- CronJobs




#### Kubernetes notes
- namespace:
    - used to create multiple virtual clusters within a single physical cluster
    - useful for separating environments (e.g., development, staging, production) or teams within an organization
    - resources within a namespace are isolated from those in other namespaces
    - default namespace is "default"
    - can create custom namespaces using YAML files or kubectl commands
- pod: 
    - smallest and simplest Kubernetes object
    - represents a single instance of a running process in a cluster
    - can contain one or more containers that share the same network namespace and storage
    - pods are ephemeral and can be created, destroyed, and recreated as needed
    - managed by higher-level controllers like Deployments or StatefulSets for scalability and availability   
- labels:
    - key-value pairs attached to Kubernetes objects (e.g., pods, services, deployments)
    - used for organizing, selecting, and filtering resources
    - facilitate grouping and managing resources based on specific criteria
    - can be used in conjunction with selectors to perform operations on a subset of resources
    - labels are flexible and can be added or modified at any time without affecting the underlying resources
- Annotations:
    - key-value pairs attached to Kubernetes objects
    - used to store non-identifying metadata that can be used by tools and libraries
    - typically used for configuration or operational information that is not relevant for selection or grouping
    - unlike labels, annotations are not used for selecting or filtering resources
    - can be added or modified at any time without affecting the underlying resources 
    - special characters are allowed in annotation keys and values, making them suitable for storing larger or more complex data   
## Annotations vs labels
| Feature        | Labels                                           | Annotations                                     |
|----------------|--------------------------------------------------|-------------------------------------------------|
| Purpose        | Used for identifying and grouping resources      | Used for storing non-identifying metadata       |
| Selection      | Can be used with selectors to filter resources   | Not used for selection or filtering             |
| Metadata Type  | Key-value pairs                                  | Key-value pairs                                 | 
| Use Cases      | Organizing resources, managing deployments       | Storing configuration or operational information|
| Modification   | Can be added or modified at any time             | Can be added or modified at any time            |    
| Special Chars  | Limited to alphanumeric characters, '-', '_'     | Allows special characters, larger data          |

- pod/container resources:
    - requests:
        - specify the minimum amount of CPU and memory resources required for a container to run
        - used by the Kubernetes scheduler to determine which node can accommodate the pod
        - ensures that the container has enough resources to function properly
    - limits:
        - specify the maximum amount of CPU and memory resources that a container can use
        - prevent a container from consuming excessive resources and impacting other containers on the same node
        - if a container exceeds its resource limits, it may be throttled or terminated by the kubelet
    - it is important to set appropriate resource requests and limits to ensure optimal performance and stability of applications running in Kubernetes

- Config maps:
    - used to store non-confidential configuration data in key-value pairs
    - allow separation of configuration from application code, making it easier to manage and update configurations without modifying the application
    - can be created using YAML files or kubectl commands
    - can be consumed by pods as environment variables, command-line arguments, or mounted as files in a volume
    - useful for managing application settings, feature flags, and other configuration data that may change frequently    

- Secrets:
    - used to store sensitive information such as passwords, API keys, and certificates
    - data is stored in an encoded format (base64) to provide a basic level of obfuscation
    - can be created using YAML files or kubectl commands
    - can be consumed by pods as environment variables or mounted as files in a volume
    - help enhance security by keeping sensitive data separate from application code and configuration

- Services:
They are used to achieve pod communication and load balancing in a Kubernetes cluster and expose applications running on a set of pods. Key points about services:
    - an abstraction that defines a logical set of pods and a policy to access them
    - provide stable IP addresses and DNS names for accessing pods, even as the underlying pods are created and destroyed
    - enable load balancing and service discovery within the cluster
    - can be exposed externally to allow access from outside the cluster
    - use selectors to identify the pods that belong to the service based on labels
There are four main types of services in Kubernetes:
    - ClusterIP (default): For internal communication within the cluster
        - exposes the service on a cluster-internal IP
        - accessible only within the cluster
        - used for communication between pods within the cluster
    - NodePort: For external access to the service
        - exposes the service on each node's IP at a static port (the NodePort)
        - accessible from outside the cluster using <NodeIP>:<NodePort>
        - useful for development and testing purposes
    - LoadBalancer: For external access with load balancing
        - provisions an external load balancer (if supported by the cloud provider)
        - exposes the service to the internet using a public IP address
        - routes traffic to the appropriate pods based on load balancing algorithms
    - ExternalName:
        - maps the service to an external DNS name
        - does not create a traditional service with selectors or endpoints
        - useful for integrating with external services outside the cluster    

- Replicasets:
    - ensure that a specified number of pod replicas are running at any given time
    - monitor the state of pods and create or delete pods as needed to maintain the desired number of replicas
    - provide high availability and fault tolerance for applications by distributing pod replicas across multiple nodes
    - can be created and managed using YAML files or kubectl commands
    - often used in conjunction with Deployments, which provide additional features such as rolling updates and rollbacks

- Deployment:
    - a higher-level abstraction that manages ReplicaSets and provides declarative updates to applications
    - allows you to define the desired state of your application, including the number of replicas, container images, and update strategies
    - automatically creates and manages ReplicaSets to ensure that the desired number of pod replicas are running
    - supports rolling updates, allowing you to update your application with zero downtime by gradually replacing old pods with new ones
    - provides rollback capabilities, enabling you to revert to a previous version of your application if needed
    - can be created and managed using YAML files or kubectl commands  

- Volumes:
    - provide persistent storage for pods in a Kubernetes cluster
    - allow data to persist beyond the lifecycle of individual pods, enabling stateful applications
    - can be used to share data between containers within the same pod
    - support various types of storage backends, including local storage, network-attached storage, and cloud-based storage solutions
    - can be defined in pod specifications and mounted into containers at specified paths
    - help ensure data durability and availability for applications running in Kubernetes

    - There are several types of volumes in Kubernetes, including:
        - emptyDir: A temporary directory that is created when a pod is assigned to a node and exists as long as the pod is running. It is useful for sharing data between containers in the same pod. For example , a web server container and a logging container can share log files using an emptyDir volume.
        - hostPath: Mounts a file or directory from the host node's filesystem into the pod. Useful for accessing host resources or sharing data between pods on the same node.
        - persistent Volume: Represents a piece of storage in the cluster that has been provisioned by an administrator or dynamically provisioned using StorageClasses. It is used to provide persistent storage for pods.
        - persistentVolumeClaim: Uses a PersistentVolumeClaim to request storage from a PersistentVolume and mount it into a pod. It is dynamically provisioned based on the claim's specifications.
        - configMap: Mounts a ConfigMap as a volume, allowing pods to consume configuration data
        - secret: Mounts a Secret as a volume, enabling pods to access sensitive information securely
        - nfs, awsElasticBlockStore, gcePersistentDisk, and others: Support for various external storage systems        

- Horizontal Pod Autoscaler (HPA):
    - automatically scales the number of pod replicas in a deployment or replica set based on observed CPU utilization or other select metrics
    - helps ensure that applications can handle varying levels of traffic and workload by adjusting the number of running pods as needed
    - can be configured using YAML files or kubectl commands
    - supports custom metrics in addition to CPU utilization for scaling decisions
    - improves application availability and performance by dynamically adapting to changing resource demands
    - requires the Metrics Server to be deployed in the cluster to collect resource usage data
    - can be created using the `kubectl autoscale` command or by defining an HPA resource in a YAML file
    - allows you to specify minimum and maximum replica counts, as well as target CPU utilization or custom metrics for scaling decisions
    - helps optimize resource usage and costs by scaling down when demand is low and scaling up when demand increases
    - requires proper resource requests and limits to be set on the pods for effective scaling

 - Vertical Pod Autoscaler (VPA):
    - automatically adjusts the CPU and memory resource requests and limits for containers in pods based on observed usage
    - helps ensure that applications have the appropriate resources to operate efficiently without over-provisioning or under-provisioning
    - can be configured using YAML files or kubectl commands
    - supports three modes: "Off" (no adjustments), "Auto" (automatic adjustments), and "Recreate" (restarts pods to apply new resource settings)
    - improves application performance and stability by adapting to changing resource demands over time
    - can be used in conjunction with Horizontal Pod Autoscaler (HPA) for comprehensive scaling strategies
    - requires the VPA admission controller to be enabled in the cluster for automatic resource adjustments
    - helps optimize resource usage and costs by ensuring that pods have the right amount of resources based on actual usage patterns   



   
    


#### Errors
- CrashLoopBackOff:
    - indicates that a container in a pod is repeatedly crashing and restarting
    - common causes include application errors, misconfigurations, or resource constraints
    - to troubleshoot, check the container logs using `kubectl logs <pod-name>` and inspect the pod description with `kubectl describe pod <pod-name>`
    - ensure that the container image is correct and that all required dependencies are available
    - consider adding readiness and liveness probes to monitor the health of the application within the container
- ImagePullBackOff:
    - indicates that Kubernetes is unable to pull the specified container image from the container registry
    - common causes include incorrect image names, missing tags, or authentication issues with private registries
    - to troubleshoot, check the pod description with `kubectl describe pod <pod-name>` to see detailed error messages related to image pulling
    - verify that the image name and tag are correct and that the container registry is accessible
    - if using a private registry, ensure that the necessary image pull secrets are configured in the pod specification



## Useful stuff

- liveness and readiness probes:
    - used to monitor the health and availability of containers within a pod
    - liveness probes determine if a container is running and healthy; if the probe fails, the container is restarted
    - readiness probes determine if a container is ready to accept traffic; if the probe fails, the pod is removed from service endpoints
    - can be configured using various methods such as HTTP requests, TCP socket checks, or command execution
    - help improve application reliability and availability by ensuring that only healthy containers receive traffic and that unhealthy containers are automatically recovered
liveness prove vs readiness probe:
| Feature            | Liveness Probe                               | Readiness Probe                                     |
|--------------------|----------------------------------------------|-----------------------------------------------------|
| Purpose            | Checks if the container is alive and healthy | Checks if the container is ready to serve traffic.  |
| Action on Failure  | Restarts the container                       | Removes the pod from service endpoints              |
| Use Case           | Detects and recovers from application crashes| Manages traffic routing based on container readiness|
| Configuration      | Can use HTTP, TCP, or command execution      | Can use HTTP, TCP, or command execution             |
| Impact on Traffic  | No direct impact on traffic routing          | Directly affects traffic routing to the pod         |




- kubectx and kubens:
    - kubectx is a command-line tool that simplifies switching between multiple Kubernetes contexts (clusters)
    - kubens is a command-line tool that simplifies switching between multiple Kubernetes namespaces within a cluster
    - both tools enhance productivity by allowing users to quickly change their working context or namespace without manually editing the kubeconfig file
    - can be installed using package managers like Homebrew or by downloading precompiled binaries from their respective GitHub repositories
    - once installed, users can switch contexts with `kubectx <context-name>` and switch namespaces with `kubens <namespace-name>`


- configmap vs secret:
| Feature        | ConfigMap                                      | Secret                                           |
|----------------|------------------------------------------------|-------------------------------------------------|
| Purpose        | Stores non-confidential configuration data     | Stores sensitive information                     |
| Data Encoding  | Plain text                                     | Base64 encoded                                  |
| Use Cases      | Application settings, feature flags            | Passwords, API keys, certificates                  |
| Access Methods | Environment variables, volume mounts           | Environment variables, volume mounts             |
| Security       | Not encrypted, suitable for non-sensitive data | Provides basic obfuscation, suitable for sensitive data |   

when config mapchanged, pods or deployments using it need to be restarted to pick up the changes. Secrets behave similarly. 


- EBS 
    - To mount an EBS volume to a pod, you need to create a PersistentVolume (PV) and a PersistentVolumeClaim (PVC) that references the EBS volume.
      Then you can mount the PVC to the pod in the pod specification.
    - EBS volumes are specific to a single availability zone, so ensure that your pods are scheduled in the same zone as the EBS volume.
    - EBS volumes can be dynamically provisioned using StorageClasses, allowing for automatic creation and management of EBS volumes based on PVC requests.
    - Install the AWS EBS CSI driver in your Kubernetes cluster to enable dynamic provisioning and management of EBS volumes.
        kubectl apply -k "github.com/kubernetes-sigs/aws-ebs-csi-driver/deploy/kubernetes/overlays/stable/?ref=release-1.53"
    - EC2 instances (nodes) must have the necessary IAM permissions to access and manage EBS volumes.
    - Create PV 

- EFS
    - To mount an EFS volume to a pod, you need to create a PersistentVolume (PV) and a PersistentVolumeClaim (PVC) that references the EFS file system.
      Then you can mount the PVC to the pod in the pod specification.
    - EFS volumes are accessible across multiple availability zones, making them suitable for applications that require shared storage.
    - Install the AWS EFS CSI driver in your Kubernetes cluster to enable dynamic provisioning and management of EFS volumes.
        kubectl kustomize \
    "github.com/kubernetes-sigs/aws-efs-csi-driver/deploy/kubernetes/overlays/stable/?ref=release-2.2" > public-ecr-driver.yaml
    - EC2 instances (nodes) must have the necessary IAM permissions to access and manage EFS file systems.
    - Create PV and PVC for EFS and mount it to pods as needed.
    - Create a security group that allows NFS traffic (port 2049) between the EFS file system and the EC2 instances (nodes) in your Kubernetes cluster.

- EBS vs EFS:
| Feature               | EBS (Elastic Block Store)                     | EFS (Elastic File System)                     |
|-----------------------|-----------------------------------------------|-------------------------------------------------|
| Storage Type          | Block storage                                 | File storage                                   |
| Accessibility         | Accessible by a single EC2 instance at a time | Accessible by multiple EC2 instances simultaneously |
| Use Cases             | Databases, file systems, applications requiring low-latency access | Shared file storage, content management systems, home directories |
| Performance           | High performance for single-instance workloads | Scalable performance for multi-instance workloads |
| Durability            | Data is replicated within a single availability zone | Data is replicated across multiple availability zones |
| Cost                  | Generally lower cost for single-instance use cases | Generally higher cost due to multi-instance access and scalability | 
| Mounting in Pods     | Requires PV and PVC for mounting               | Requires PV and PVC for mounting                 |
| Dynamic Provisioning | Supported with AWS EBS CSI driver                | Supported with AWS EFS CSI driver                |
| IAM Permissions      | Required for EC2 instances to manage EBS volumes | Required for EC2 instances to manage EFS file systems |
| Performance Modes     | General Purpose (gp2, gp3), Provisioned IOPS (io1, io2) | Bursting Throughput, Provisioned Throughput      |
| Scalability          | Up to 16 TiB per volume                        | Scales up to petabytes automatically            |
| Backup and Snapshots | Supports snapshots for data backup and recovery | Supports backups through AWS Backup service      |
| Data Sharing        | Not designed for data sharing between instances | Designed for concurrent access by multiple instances |
| Integration with AWS Services | Integrates with EC2, RDS, and other services | Integrates with EC2, ECS, EKS, and other services |
| Performance Consistency | Provides consistent performance for single-instance workloads | Performance may vary based on the number of concurrent connections and workload |
| Security            | Supports encryption at rest and in transit     | Supports encryption at rest and in transit       |
| Security groups      | EBS volumes inherit the security groups of the EC2 instance | EFS file systems can be associated with specific security groups |        


#### sets 

- Replica set:
    - ensures that a specified number of pod replicas are running at any given time
    - monitors the state of pods and creates or deletes pods as needed to maintain the desired number of replicas
    - provides high availability and fault tolerance for applications by distributing pod replicas across multiple nodes
    - can be created and managed using YAML files or kubectl commands
    - often used in conjunction with Deployments, which provide additional features such as rolling updates and rollbacks

- Deployment:
    - a higher-level abstraction that manages ReplicaSets and provides declarative updates to applications
    - allows you to define the desired state of your application, including the number of replicas, container images, and update strategies
    - automatically creates and manages ReplicaSets to ensure that the desired number of pod replicas are running
    - supports rolling updates, allowing you to update your application with zero downtime by gradually replacing old pods with new ones
    - provides rollback capabilities, enabling you to revert to a previous version of your application if needed
    - can be created and managed using YAML files or kubectl commands
    - It is for stateless applications where any pod can handle any request

- DaemonSets:
    - ensure that a copy of a specific pod is running on all (or a subset of) nodes in a Kubernetes cluster
    - commonly used for deploying system-level services such as log collectors, monitoring agents, or network plugins or logging agents.
    - automatically adds pods to new nodes as they are added to the cluster and removes pods from nodes that are removed
    - can be created and managed using YAML files or kubectl commands
    - provide a way to ensure that essential services are consistently available across all nodes in the cluster    
    - logs are accessed by hostPath volume or DaemonSet pods can forward logs to a centralized logging system

 - StatefulSets:
    - manage the deployment and scaling of stateful applications in Kubernetes
    - provide guarantees about the ordering and uniqueness of pod instances
    - maintain a stable network identity and persistent storage for each pod, even when pods are rescheduled or restarted
    - commonly used for applications that require stable identities and persistent data, such as databases or distributed systems
    - can be created and managed using YAML files or kubectl commands   
    - It is for stateful applications where each pod has its own identity and storage
    - A Headless service is mandatory to be created for a statefulset to function properly.

- Headless Services:
    - a type of Kubernetes service that does not allocate a cluster IP address
    - used to expose pods directly without load balancing or proxying
    - commonly used in conjunction with StatefulSets to provide stable network identities for each pod
    - allows clients to connect directly to individual pod instances using their DNS names
    - can be created and managed using YAML files or kubectl commands   
    - useful for stateful applications that require direct access to specific pod instances, such as databases or clustered applications
    - When a headless service is created, Kubernetes does not assign it a cluster IP address. Instead, it creates DNS records for each pod that matches the service's selector. This allows clients to resolve the DNS name of the headless service to the individual pod IP addresses, enabling direct communication with specific pod instances.
    - when a headless sdervie is hit, it will give all the pod IPs. It is for data repplication and clustering of stateful applications(like DBs  etc).
    - It is mandatory to create a headless service for statefulsets. 


Deployment vs Statefulset vs Daemonset:
| Feature               | Deployment                                  | StatefulSet                                   | DaemonSet                                    |
|-----------------------|---------------------------------------------|-----------------------------------------------|----------------------------------------------|
| Purpose               | Manages stateless applications               | Manages stateful applications                  | Ensures a pod runs on all or specific nodes |
| Pod Identity          | Pods are interchangeable                     | Each pod has a unique identity and stable network ID | Pods run on all or specific nodes            |
| Storage               | Typically uses shared storage or no storage  | Provides stable, persistent storage for each pod | Can use hostPath or other storage types       |
| Scaling               | Easy to scale up or down                      | Scaling maintains pod identity and order       | Not typically scaled; runs on all nodes         |
| Update Strategy       | Supports rolling updates and rollbacks       | Supports rolling updates with ordered pod termination | Updates can be applied to all pods simultaneously |
| Use Cases             | Web servers, APIs, front-end applications | Databases, distributed systems, stateful apps | Log collectors, monitoring agents, network plugins |
| Network Identity      | Pods share the same network identity         | Each pod has a unique network identity         | Pods share the same network identity          |
| Pod Scheduling      | Pods can be scheduled on any node            | Pods are scheduled based on their identity and storage | Pods are scheduled on all or specific nodes   |    


- Volumes:
    - provide persistent storage for pods in a Kubernetes cluster
    - allow data to persist beyond the lifecycle of individual pods, enabling stateful applications
    - can be used to share data between containers within the same pod
    - support various types of storage backends, including local storage, network-attached storage, and cloud-based storage solutions
    - can be defined in pod specifications and mounted into containers at specified paths
    - help ensure data durability and availability for applications running in Kubernetes
  - Static provisioning:
      - An administrator manually creates PersistentVolumes (PVs) in the cluster.
      - Users create PersistentVolumeClaims (PVCs) to request storage from the available PVs.
      - The PVC is bound to a matching PV based on size and access modes.
  - Dynamic provisioning:
      - Storage class is created by the administrator to define different types of storage (e.g., SSD, HDD) and their parameters.
      - Users create PVCs without pre-existing PVs.
      - The cluster automatically provisions a new PV based on the PVC's specifications using StorageClasses. 


 Scheduling in Kubernetes:
  - Kubernetes uses a scheduler to assign pods to nodes based on resource availability and other constraints.
  - The scheduler considers factors such as CPU and memory requests, node affinity/anti-affinity, taints and tolerations, and custom scheduling policies.
  - The default scheduler is kube-scheduler, but custom schedulers can also be implemented for specific use cases.
  - Scheduling decisions are made based on a two-step process: filtering and scoring.
    - Filtering: The scheduler filters out nodes that do not meet the pod's requirements (e.g., insufficient resources, taints).
    - Scoring: The scheduler scores the remaining nodes based on various criteria (e.g., resource utilization, affinity rules) and selects the best-fit node for the pod.
  - Users can influence scheduling decisions by specifying resource requests and limits, node selectors, affinity rules, and tolerations in the pod specification.  
- Taints and Tolerations:
    - Taints are applied to nodes to repel certain pods from being scheduled on them.
    - Tolerations are applied to pods to allow them to be scheduled on nodes with matching taints.
    - Taints and tolerations work together to control pod placement and ensure that specific workloads are isolated or prioritized on certain nodes.
    - Common use cases include dedicating nodes for specific workloads, isolating critical applications, or preventing certain pods from running on specific nodes.
    - Taints can be added to nodes using kubectl commands or defined in node specifications, while tolerations are specified in pod specifications.

    - Adding tolerations to pods allows them to be scheduled on nodes with matching taints, but its not guraranteed to be scheduled only in the tainted node(so use nodeselector), while nodes without the appropriate tolerations will repel those pods.
    - NoSchedule effect: Pods that do not tolerate the taint will not be scheduled on the node.
    - PreferNoSchedule effect: The scheduler will try to avoid placing pods that do not tolerate the taint on the node, but it is not guaranteed.
    - NoExecute effect: Pods that do not tolerate the taint will be evicted from the node if they are already running there. If a pod tolerates the taint, it can continue to run on the node till tolerationSeconds expires, if no tolerationSeconds is set, it can run indefinitely.
      

    taints vs tolerations:
| Feature        | Taints                                         | Tolerations                                     |
|----------------|------------------------------------------------|-------------------------------------------------|
| Purpose        | Repel pods from being scheduled on nodes       | Allow pods to be scheduled on nodes with matching taints |
| Applied To    | Nodes                                          | Pods                                            |
| Use Cases      | Isolating workloads, dedicating nodes          | Allowing specific pods to run on tainted nodes |
| Configuration  | Added to nodes using kubectl or node specs      | Specified in pod specifications                 |
| Interaction     | Work together with tolerations to control pod placement | Work together with taints to control pod placement |
| Example        | Taint a node to repel all pods except those with matching tolerations | Tolerate a specific taint to allow scheduling on tainted nodes |
| Effect on Scheduling | Prevents pods without matching tolerations from being scheduled on the node | Allows pods with matching tolerations to be scheduled on the node |
| Impact on Workloads | Helps ensure that specific workloads are isolated or prioritized on certain nodes | Helps ensure that specific workloads can run on nodes with specific taints |
| Configuration Syntax | `kubectl taint nodes <node-name> key=value:effect` | Specified in pod YAML under `tolerations` field |
| Examples       | `kubectl taint nodes node1 dedicated=group1:NoSchedule` | `tolerations: - key: "dedicated" operator: "Equal" value: "group1" effect: "NoSchedule"` |    



- Affinity and Anti-Affinity:
    - Affinity rules specify preferences for scheduling pods on nodes based on labels, allowing pods to be co-located on the same node or in the same zone.
    - Anti-affinity rules specify preferences for avoiding scheduling pods on nodes with certain labels, helping to distribute workloads across nodes or zones.
    - Both affinity and anti-affinity can be defined at the pod level using node affinity (for node selection) or pod affinity/anti-affinity (for co-locating or separating pods).
    - Affinity and anti-affinity rules can be specified as required (hard constraints) or preferred (soft constraints), influencing the scheduler's decisions without being mandatory.
    - Common use cases include improving application performance by co-locating related pods, enhancing fault tolerance by distributing pods across failure domains, and optimizing resource utilization by balancing workloads across nodes.
    - Affinity and anti-affinity rules are defined in the pod specification under the `affinity` field.
    - Node Affinity: Used to constrain which nodes a pod can be scheduled on based on node labels.
    - Pod Affinity: Used to co-locate pods on the same node or in the same zone based on pod labels.
    - Pod Anti-Affinity: Used to avoid co-locating pods on the same node or in the same zone based on pod labels.
| Feature        | Affinity                                      | Anti-Affinity                                  |
|----------------|-----------------------------------------------|------------------------------------------------|
| Purpose        | Specifies preferences for scheduling pods on nodes based on labels | Specifies preferences for avoiding scheduling pods on nodes with certain labels |
| Types          | Node Affinity, Pod Affinity                     | Pod Anti-Affinity                              |
| Use Cases      | Co-locating related pods, improving performance | Distributing workloads, enhancing fault tolerance |
| Configuration  | Defined in pod specification under `affinity` field | Defined in pod specification under `affinity` field |
| Interaction     | Works together with anti-affinity to control pod placement | Works together with affinity to control pod placement |
| Effect on Scheduling | Influences scheduler to place pods on preferred nodes or with preferred pods | Influences scheduler to avoid placing pods on certain nodes or with certain pods |
| Impact on Workloads | Helps optimize resource utilization and application performance | Helps ensure workload distribution and fault tolerance |
| Configuration Syntax | Specified in pod YAML under `affinity` field | Specified in pod YAML under `affinity` field |
| Examples       | Node Affinity: `requiredDuringSchedulingIgnoredDuringExecution` | Pod Anti-Affinity: `preferredDuringSchedulingIgnoredDuringExecution` |
| Pod Affinity: |`requiredDuringSchedulingIgnoredDuringExecution` |
| Pod Anti-Affinity: | `requiredDuringSchedulingIgnoredDuringExecution` |   

     - node affinity:
        - Its an enhanced version of nodeSelector.
        - allows more expressive rules for pod scheduling based on node labels.
        - has more operators like In, NotIn, Exists, DoesNotExist, Gt, Lt.
        - requiredDuringSchedulingIgnoredDuringExecution: Pods must be scheduled on nodes that match the specified label criteria. hard requirement. pod status pending if not matched.
        - preferredDuringSchedulingIgnoredDuringExecution: Pods prefer to be scheduled on nodes that match the specified label criteria, but it is not mandatory. soft requirement. pod can be scheduled on non-matching nodes if necessary.
           - multiple preferences can be specified with different weights to influence scheduling decisions. 
           - the scheduler will try to place the pod on nodes that satisfy the highest-weighted preferences first.
        - Not<operator>: Used to specify that a node should not have certain labels for the pod to be scheduled there. It is anti-affinity at node level.
    
    - pod affinity:
        - allows pods to be co-located on the same node or in the same zone based on pod labels.
        - has In, NotIn, Exists, DoesNotExist as operators.
        - used to improve application performance by placing related pods together.
        - requiredDuringSchedulingIgnoredDuringExecution: Pods must be scheduled on nodes that already have pods with the specified labels. hard requirement.
        - preferredDuringSchedulingIgnoredDuringExecution: Pods prefer to be scheduled on nodes that already have pods with the specified labels, but it is not mandatory. soft requirement.
           - multiple preferences can be specified with different weights to influence scheduling decisions.
           - the scheduler will try to place the pod on nodes that satisfy the highest-weighted preferences first.
        - Not<operator>: Used to specify that a pod should not be co-located with certain pods for the pod to be scheduled there. It is anti-affinity at pod level.   





#### Helm charts.
- Helm:
    - a package manager for Kubernetes that simplifies the deployment and management of applications
    - uses charts, which are pre-configured packages of Kubernetes resources, to define, install, and upgrade applications
    - allows users to easily share and distribute applications through Helm repositories
    - provides features such as templating, versioning, and dependency management for Kubernetes applications
    - can be used to deploy complex applications with multiple components and configurations using a single command
    - helps streamline the application lifecycle management process in Kubernetes environments
- Helm Charts:
    - a collection of files that describe a related set of Kubernetes resources
    - typically include templates for Kubernetes manifests, configuration values, and metadata about the chart
    - can be customized using values files or command-line arguments during installation or upgrade
    - can be stored in Helm repositories for easy sharing and distribution
    - allow for easy installation, upgrade, and rollback of applications in Kubernetes clusters
    - can define dependencies on other charts, enabling the deployment of complex applications with multiple components       

    - Installing Helm:
        - Download the Helm binary from the official Helm website or GitHub repository.
        - Extract the binary and move it to a directory in your system's PATH (e.g., /usr/local/bin).
        - Verify the installation by running `helm version` in your terminal.
            # curl -fsSL -o get_helm.sh https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-4
            # chmod 700 get_helm.sh
            # ./get_helm.sh
    - Adding Helm Repositories:
        - Use the `helm repo add` command to add a Helm repository. For example:
          `helm repo add stable https://charts.helm.sh/stable`
        - Update the local Helm repository cache using `helm repo update`.
    - Installing Helm Charts:
        - Use the `helm install` command to install a Helm chart. For example:
          `helm install my-release stable/nginx`
        - You can customize the installation by providing a values file using the `-f` flag or by setting individual values using the `--set` flag.
    - Managing Helm Releases:
        - Use the `helm list` command to view installed Helm releases.
        - Upgrade a release using the `helm upgrade` command. For example:
          `helm upgrade my-release stable/nginx`
        - Rollback a release to a previous version using the `helm rollback` command. For example:
          `helm rollback my-release 1`  
        - Uninstall a release using the `helm uninstall` command. For example:
          `helm uninstall my-release`
        - View the status of a release using the `helm status` command. For example:
          `helm status my-release`

    - Hemlo commands:
        - `helm create <chart-name>`: Creates a new Helm chart with the specified name.
        - `helm lint <chart-path>`: Validates a Helm chart for correctness and best practices.
        - `helm package <chart-path>`: Packages a Helm chart into a .tgz file for distribution.
        - `helm search repo <keyword>`: Searches for charts in the configured Helm repositories.
        - `helm get all <release-name>`: Retrieves all information about a specific Helm release.
        - `helm history <release-name>`: Displays the revision history of a specific Helm release.
        - `helm install <chart-name>  Installs a Helm chart with a generated release name.      
        - `helm install <chart-name> .  Installs a Helm chart 
        - `helm install <release-name> <chart-name>`: Installs a Helm chart with a specified release name.
        - `helm upgrade <release-name> <chart-name>`: Upgrades an existing Helm
        - `helm rollback <release-name> <revision-number>`: Rolls back a Helm release to a specific revision.
        - `helm list`: Lists all installed Helm releases in the current namespace.
        - `helm uninstall <release-name>`: Uninstalls a Helm release from the cluster.
        - `helm repo add <repo-name> <repo-url>`: Adds a Helm repository to the local configuration.
        - `helm repo update`: Updates the local Helm repository cache with the latest charts from the configured
        - `helm search repo <keyword>`: Searches for Helm charts in the configured repositories using the specified keyword.
        - `helm show values <chart-name>`: Displays the default values for a specified Helm chart.
        - `helm template <chart-name>`: Renders the templates of a Helm chart locally without installing it.
        - `helm lint <chart-path>`: Validates a Helm chart for correctness and best practices.    
        - `helm install <release-name> <chart-name> -f values-dev.yaml`: Installs a Helm chart with a specified release name using a custom values file for configuration.




#### RBAC 
- Role-Based Access Control (RBAC):
    - a method for regulating access to resources in a Kubernetes cluster based on the roles of individual users or groups
    - allows administrators to define roles with specific permissions and assign those roles to users or groups
    - helps ensure that users have the appropriate level of access to perform their tasks while maintaining security and compliance
    - can be configured using YAML files or kubectl commands
    - supports four main RBAC resources: Role, ClusterRole, RoleBinding, and ClusterRoleBinding
    - enables fine-grained control over access to Kubernetes resources, such as pods, services, and namespaces
- RBAC Resources:
    - Role: Defines a set of permissions within a specific namespace. It specifies what actions (verbs) can be performed on which resources. Namespace level
    - ClusterRole: Similar to Role, but it applies cluster-wide, allowing permissions to be granted across all namespaces. Cluster level
    - RoleBinding: Binds a Role to a user or group within a specific namespace, granting them the permissions defined in the Role. Namespace level
    - ClusterRoleBinding: Binds a ClusterRole to a user or group at the cluster level, granting them the permissions defined in the ClusterRole across all namespaces. Cluster level
- RBAC Concepts:
    - Subjects: The users, groups, or service accounts that are granted permissions through RoleBindings or ClusterRoleBindings.
    - Verbs: The actions that can be performed on resources, such as get, list, create, update, delete, etc.
    - Resources: The Kubernetes objects that permissions can be granted for, such as pods, services, deployments, etc.
    - Namespaces: Logical partitions within a Kubernetes cluster that help organize and isolate resources. Roles and RoleBindings are namespace-specific, while ClusterRoles and ClusterRoleBindings are cluster-wide.
    - Policy Rules: Define the specific permissions granted by a Role or ClusterRole, specifying the verbs and resources that can be accessed.
    - Aggregation: Allows combining multiple ClusterRoles into a single ClusterRole, simplifying permission management for complex scenarios.
    - Default Roles: Kubernetes provides several built-in ClusterRoles, such as cluster-admin, admin, edit, and view, which can be used to quickly assign common permission sets to users or groups.
    - Auditing: RBAC actions can be audited to track who accessed what resources and when, helping with security and compliance monitoring.
    - Best Practices:
        - Follow the principle of least privilege by granting only the necessary permissions to users and groups.
        - Regularly review and update RBAC policies to ensure they align with changing requirements and security practices.
        - Use namespaces to isolate resources and limit the scope of Roles and RoleBindings.
        - Leverage ClusterRoles for common permission sets that need to be applied across multiple namespaces.
        - Document RBAC policies and changes to maintain clarity and accountability within the team.
        - Test RBAC configurations in a non-production environment before applying them to production clusters.
        - Use tools like `kubectl auth can-i` to verify permissions for users and service accounts.
        - Monitor and audit RBAC activities to detect and respond to unauthorized access attempts.
    - Example RBAC YAML:
        ```yaml
        apiVersion: rbac.authorization.k8s.io/v1
        kind: Role
        metadata:
          namespace: my-namespace
          name: pod-reader
        rules:
        - apiGroups: [""]
          resources: ["pods"]
          verbs: ["get", "watch", "list"]


#### How to achieve RBAC?
    -  Lets say a user is created in the system. To provide access to the kubernetes cluster, we need to create a role/clusterrole and rolebinding/clusterrolebinding.
        -  A user named suresh is created in the system. he is a jr.devops engineer so he only has read permissions.
        -  Now give IAM permission to suresh to describe a cluster, lets say the Roboshop cluster. [ Cluster describe policy is to be created and attached to the user suresh ]
        -  Attach the policy to suresh user.
        -  Now make changes in aws-auth config map to map the IAM user suresh to a kubernetes user suresh.
        -  Now create a role named roboshop-trainee with read only permissions to pods in default namespace and apply the role. This will be for a specific name space . see role.yaml for reference.    
        -  Now bind that role to the user suresh by creating a rolebinding and apply it. see rolebinding.yaml for reference.    
        -  Now take the aws-auth config map from the kubesystem namespace and add the following lines in mapUsers section.{ aws-auth is the configmap whereEKS and IAM are integrated. It is in kube-system namespace.}
        -  It can be accessed by kubectl get configmap aws-auth -n kube-system -o yaml
        -  Manually update the configmap with the users and groups that are required.
        -  Apply it. Now make sure to apply the config map first and then the role and rolebinding.

    -  Lets say Ramesh is the new user and he is a team leader    
        -  Follow the same steps as above to create the user in IAM and provide cluster describe permissions.
        -  Now create a role named roboshop-admin with admin permissions to all namespaces and apply the role. see adminrole.yaml for reference.    
        -  Now create a clusterrole named roboshop-admin with admin permissions to all namespaces and apply the role. see adminrole.yaml for reference.  
        -  Now bind that role to the user ramesh by creating a rolebinding and apply it. see adminrolebinding.yaml for reference.    
        -  Now bind that clusterrole to the user ramesh by creating a clusterrolebinding and apply it. see adminrolebinding.yaml for reference.
        -  Now take the aws-auth config map from the kubesystem namespace and add the following lines in mapUsers section.
        -  It can be accessed by kubectl get configmap aws-auth -n kube-system -o yaml
        -  Manually update the configmap with the users and groups that are required.
        -  Apply it. Now make sure to apply the config map first and then the role and rolebinding.

    -  Lets say we have a users group named roboshop-trainee and all the users in that group should have view permissions to roboshop-dev namespace.
        -  Create a group named roboshop-trainee in IAM and add all the users to that group.
        -  Provide cluster describe permissions to that group.
        -  Now create a role named roboshop-trainee with view permissions to roboshop-dev namespace and apply the role. see grouprole.yaml for reference.    
        -  Now bind that role to the group roboshop-trainee by creating a rolebinding and apply it. see grouprolebinding.yaml for reference.    
        -  Now take the aws-auth config map from the kubesystem namespace and add the following lines in mapUsers section.
        -  It can be accessed by kubectl get configmap aws-auth -n kube-system -o yaml
        -  Manually update the configmap with the users and groups that are required.
        -  Apply it. Now make sure to apply the config map first and then the role and rolebinding.

 

    -  Service account
        - a special type of Kubernetes account used by pods to authenticate with the Kubernetes API server
        - allows pods to securely access cluster resources and perform actions on behalf of the application running inside the pod
        - automatically created for each namespace, with a default service account named "default"
        - can be customized by creating additional service accounts with specific permissions and roles
        - used in conjunction with RBAC to control access to cluster resources for pods
        - can be specified in pod specifications using the `serviceAccountName` field
        - helps ensure secure communication between pods and the Kubernetes API server, enabling applications to interact with cluster resources safely and efficiently  
        - OIDC (Open ID Connect) based authentication is used by service accounts. It allows Kubernetes api server to validate user identities via an external identity provider.
        - Service accounts are associated with a set of credentials (tokens) that are automatically mounted into the pods using the service account.
        - These tokens are used to authenticate the pod with the Kubernetes API server.
        - By default, service accounts have limited permissions. To grant specific permissions to a service account, you can create Roles or ClusterRoles and bind them to the service account using RoleBindings or ClusterRoleBindings.     
        - It can also access other aws services if the IAM role is associated with the service account using IAM Roles for Service Accounts (IRSA) 

        - Steps to create a service account, but before enable OIDC for the EKS cluster:
            eksctl create iamserviceaccount --name <service-account-name> --namespace <namespace> --cluster <cluster-name> --attach-policy-arn <policy-arn> --approve #can do this too --override-existing-serviceaccounts

    ## Install an OIDC provider in EKS cluster, OpenID conenct 
        - Use the AWS Management Console, AWS CLI, or eksctl to associate an OIDC provider with your EKS cluster.
        or eksctl command:
        ```
        eksctl utils associate-iam-oidc-provider --region <region> --cluster <cluster-name> --approve
        ``` 
        - This step is necessary to enable IAM Roles for Service Accounts (IRSA) in your cluster.   
        - Create an iam role(with necessary permissions) and attach to the service account with necessary permissions . 
        - The service account can then be assigned the role using OIDC provider.\
        - This allows the pods using that service account to access AWS resources securely without needing to manage long-term AWS credentials.

    -  Lets say we have a service account named roboshop-sa in roboshop-dev namespace and it should have view permissions to that namespace.
        create a secret with secrets manager in aws for example as database password.
        Create an iam role with necessary permissions to access the secret from secrets manager.
        Attach a trust relationship to the role for the OIDC provider of the EKS cluster and the service account roboshop-sa in roboshop-dev namespace.
        -  Follow the same steps as above to create the OIDC provider and iam role.
        -  Create a service account named roboshop-sa in roboshop-dev namespace. ## created with eksctl, can be done with yaml too or see serviceaccount.yaml for reference.
        -  the pod with sql is created with the service account roboshop-sa. < by default it uses default service account if not specified >
        -  Now the pod can access the secret from secrets manager using the iam role attached to the service account.



#### Ingress Controller and Ingress Resources:
- Ingress Controller:
    - By default kubernetes creates a basic load balancer for each service of type LoadBalancer. But when there are multiple services, it is not efficient to create a load balancer for each service. In such cases, Ingress controller is used to manage external access to multiple services using a single load balancer.
    - a specialized load balancer that manages external access to services within a Kubernetes cluster
    - responsible for processing Ingress resources and routing incoming traffic to the appropriate backend services based on defined rules
    - typically deployed as a pod or set of pods within the cluster, often using popular implementations such as NGINX, Traefik, or HAProxy
    - provides features such as SSL/TLS termination, path-based routing, host-based routing, and load balancing
    - can be configured using YAML files or kubectl commands
    - helps simplify the management of external access to services in a Kubernetes environment
- Ingress Resources:
    - a Kubernetes resource that defines rules for routing external HTTP and HTTPS traffic to services within the cluster
    - allows users to specify how incoming requests should be directed based on hostnames, paths, and other criteria
    - typically used in conjunction with an Ingress Controller to implement the defined routing rules
    - can be created and managed using YAML files or kubectl commands
    - provide a way to expose multiple services under a single IP address or hostname, simplifying access
    - support features such as SSL/TLS termination, path-based routing, and host-based routing
    - help streamline the management of external access to services in a Kubernetes environment
- Ingress vs Ingress Controller:
| Feature               | Ingress                                     | Ingress Controller                          |
|-----------------------|---------------------------------------------|---------------------------------------------|
| Purpose               | Defines rules for routing external traffic to services within the cluster | Manages external access to services based on Ingress resources |
| Type                  | Kubernetes resource                         | Specialized load balancer                    |
| Functionality         | Specifies routing rules based on hostnames, paths, etc. | Processes Ingress resources and routes traffic accordingly |
| Deployment            | Created and managed using YAML files or kubectl commands | Deployed as a pod or set of pods within the cluster |
| Features              | Supports SSL/TLS termination, path-based routing, host-based routing | Provides load balancing, SSL/TLS termination, and routing capabilities |
| Configuration         | Defined in Ingress resource specifications                  | Configured through Ingress Controller settings and annotations |
| Use Cases             | Exposing multiple services under a single IP address or hostname | Managing external access to services in a Kubernetes environment | 
| Interaction            | Works in conjunction with an Ingress Controller to implement routing rules | Processes and enforces the rules defined in Ingress resources |
- Common Ingress Controllers:
    - NGINX Ingress Controller: A widely used open-source Ingress Controller based on the NGINX web server, known for its performance and flexibility.
    - Traefik: A dynamic Ingress Controller that supports automatic service discovery and configuration, making it easy to manage routing rules.
    - HAProxy Ingress Controller: An Ingress Controller based on the HAProxy load balancer, known for its high performance and reliability.
    - Istio Ingress Gateway: Part of the Istio service mesh, providing advanced traffic management and security features for Ingress traffic.
    - Ambassador: An API Gateway and Ingress Controller designed for microservices, offering features such as rate limiting and authentication.
    - Contour: An Ingress Controller built on Envoy Proxy, providing advanced routing and load balancing capabilities.
    - GCE Ingress Controller: A Google Cloud-specific Ingress Controller that integrates with Google Cloud Load Balancing services.

    ##with an ALB, the flow is ALB > listner > Rule > target group > service> pod. The ingress controller creates an ALB and manages the routing of traffic to the services based on the ingress rules defined.
    
    - To install aws loadbalance rcontroller for EKS cluster: refer for detailed installations steps https://kubernetes-sigs.github.io/aws-load-balancer-controller/latest/deploy/installation/
        - install OIDC provider for the EKS cluster if not already done.{ eksctl utils associate-iam-oidc-provider --region <region> --cluster <cluster-name> --approve }
        - create an iam policy with necessary permissions for the ingress controller.
        - create an iam role and attach the policy to the role. also attach a trust relationship for the OIDC provider and the service account to be created.
        - create a service account for the ingress controller in the kube-system namespace and associate the iam role to the service account.
        - Install helm drivers 
        - install the ingress controller using helm or kubectl by specifying the service account created.
    See the documentation for clear instructions.
        - Create an IAM OIDC provider. You can skip this step if you already have one for your cluster.

        - Steps from documentation
            eksctl utils associate-iam-oidc-provider \
                --region <region-code> \
                --cluster <your-cluster-name> \
                --approve
            Download an IAM policy for the LBC using one of the following commands:

            If your cluster is in a US Gov Cloud region:

            curl -o iam-policy.json https://raw.githubusercontent.com/kubernetes-sigs/aws-load-balancer-controller/v2.17.0/docs/install/iam_policy_us-gov.json
            If your cluster is in a China region:
            curl -o iam-policy.json https://raw.githubusercontent.com/kubernetes-sigs/aws-load-balancer-controller/v2.17.0/docs/install/iam_policy_cn.json
            If your cluster is in any other region:
            curl -o iam-policy.json https://raw.githubusercontent.com/kubernetes-sigs/aws-load-balancer-controller/v2.17.0/docs/install/iam_policy.json
            Create an IAM policy named AWSLoadBalancerControllerIAMPolicy. If you downloaded a different policy, replace iam-policy with the name of the policy that you downloaded.

            aws iam create-policy \
                --policy-name AWSLoadBalancerControllerIAMPolicy \
                --policy-document file://iam-policy.json
            Take note of the policy ARN that's returned.
            Create an IAM role and Kubernetes ServiceAccount for the LBC. Use the ARN from the previous step.

            eksctl create iamserviceaccount \
            --cluster=<cluster-name> \
            --namespace=kube-system \
            --name=aws-load-balancer-controller \
            --attach-policy-arn=arn:aws:iam::<AWS_ACCOUNT_ID>:policy/AWSLoadBalancerControllerIAMPolicy \
            --override-existing-serviceaccounts \
            --region <region-code> \
            --approve

            Then add eks chart repo

            Then load balancer controller installation using helm



#### Deployment Strategies:
 
- Deployment Strategies:
    - a set of approaches for updating applications in a Kubernetes cluster while minimizing downtime and ensuring availability
    - help manage the rollout of new application versions, allowing for controlled updates and rollbacks if necessary
    - can be configured using Deployment resources in Kubernetes
    - support various strategies such as Rolling Update, Recreate, Blue-Green Deployment, and Canary Deployment
    - enable developers and operators to choose the most appropriate strategy based on application requirements and operational constraints
    - help ensure a smooth transition between application versions, reducing the risk of disruptions to end-users
- Common Deployment Strategies:
    - Rolling Update: Gradually replaces old pods with new ones, ensuring that a specified number of pods are always available during the update process. This is the default strategy in Kubernetes.
    - Recreate: Terminates all existing pods before creating new ones. This strategy may result in downtime but is simpler to implement for certain applications.
    - Blue-Green Deployment: Maintains two separate environments (blue and green) for the application. The new version is deployed to the inactive environment, and traffic is switched to it once it is verified to be working correctly.
    - Canary Deployment: Deploys the new version to a small subset of users or pods initially, allowing for testing and validation before rolling it out to the entire user base.
    - A/B Testing: Similar to Canary Deployment, but focuses on testing different versions of an application with distinct user groups to gather feedback and performance metrics.
    - Shadow Deployment: Routes a copy of the production traffic to the new version of the application without affecting the live environment, allowing for testing and validation.
    - Rolling with Pause: Similar to Rolling Update, but allows for pausing the rollout at specific intervals to monitor the new version's performance before proceeding further.
    - Custom Strategies: Organizations can also implement custom deployment strategies tailored to their specific application needs and operational requirements.


### Init containers:
- Init Containers:
    - specialized containers that run before the main application containers in a pod
    - used to perform initialization tasks, such as setting up the environment, preparing data, or performing checks
    - defined in the pod specification under the `initContainers` field
    - run sequentially, with each init container completing successfully before the next one starts
    - help ensure that the main application containers have the necessary prerequisites and configurations before they start running
    - can be used for tasks such as database migrations, configuration file generation, or waiting for external services to become available
    - provide a way to separate initialization logic from the main application logic, improving maintainability and clarity
    - can be implemented using any container image, allowing for flexibility in defining initialization tasks
    - help improve the reliability and stability of applications by ensuring that they start in a well-defined state
- Common Use Cases for Init Containers:
    - Database Migrations: Running database migration scripts to ensure that the database schema is up-to-date before the application starts.
    - Configuration Generation: Generating configuration files or environment variables based on external inputs or secrets.
    - Dependency Checks: Verifying that required services or resources are available before starting the main application.
    - Data Preparation: Downloading or preparing data that the application needs to function correctly.
    - Security Scans: Performing security checks or vulnerability scans on the application code or dependencies.
    - Resource Initialization: Setting up necessary resources, such as creating directories or setting permissions.
    - Network Setup: Configuring network settings or establishing connections to external services.
    - Custom Initialization Logic: Implementing any other custom logic required to prepare the application environment before startup.

    Refer the mysql module in Docker Roboshop for an example of init containers.  in daws86 repo. 


#### Upgrading eks cluster :
- To upgrade an EKS cluster, you can use the AWS Management Console, AWS CLI, or eksctl command-line tool. Here are the general steps to upgrade an EKS cluster using eksctl:
    1. Check the current version of your EKS cluster:
        ```
        eksctl get cluster --name <cluster-name>
        ```
    2. Review the available EKS versions and choose the desired version to upgrade to.
    3. Update the EKS cluster using eksctl:
        ```
        eksctl upgrade cluster --name <cluster-name> --version <new-version>
        ```
    4. Monitor the upgrade process and ensure that the cluster is successfully upgraded.
    5. After the cluster upgrade, you may also need to upgrade the worker nodes to match the new cluster version. You can do this by updating the node group:
        ```
        eksctl upgrade nodegroup --cluster <cluster-name> --name <node-group-name> --version <new-version>
        ```
    6. Verify that the worker nodes are successfully upgraded and running the new version.
    7. Test your applications and workloads to ensure they are functioning correctly after the upgrade.
    8. Review and update any custom configurations or add-ons that may be affected by the upgrade.
    9. Document the upgrade process and any changes made for future reference.

    # refer session 70,71,72 notes for more details. regarding init containers also.
  ## using blue green srategy.
    - first create a new node group lets say green with old version or new version of eks cluster.
    - then upgrade the eks cluster 
    - then upgrade the green node group to match the new eks cluster version
    - then switch the workloads from blue node group to green node group by cordoning and draining the blue node group.
    - then delete the blue node group if not needed. 
    - edit firewalls/security groups if needed.
    - Steps to switch workloads from blue node group to green node group:       
   
    - cordon blue node brougp > `kubectl cordon <blue-node-name>` | # means mark the node as unschedulable so that no new pods are scheduled on it.
    - untaint green nodes before draining blue node group > `kubectl taint nodes <green-node-name> upgrade=true:NoSchedule-` | # means remove the taint from the green nodes so that pods can be scheduled on them.
    - drain blue node group > `kubectl drain <blue-node-name> --ignore-daemonsets --delete-local-data --delete-emptydir-data` |  # means evict all the pods from the node except daemonsets and delete local data.
    - verify the workloads are running on green node group > `kubectl get nodes -o wide`   # means to check the nodes and their status.



    ## VPC CNI:
    - VPC CNI (Container Network Interface):
        - a networking plugin for Kubernetes that enables pods to have direct access to the underlying VPC (Virtual Private Cloud) network
        - allows pods to receive IP addresses from the VPC subnet, enabling seamless communication with other resources in the VPC
        - provides high-performance networking by leveraging the native VPC networking capabilities
        - supports features such as security groups, network policies, and VPC peering
        - can be configured using ConfigMap and environment variables
        - helps ensure that pods can communicate with other AWS services and resources securely and efficiently
        - commonly used in Amazon EKS (Elastic Kubernetes Service) clusters to provide networking for pods
        - allows for better integration with AWS services, such as load balancers and NAT gateways
        - helps improve the scalability and reliability of Kubernetes applications running in AWS environments
    The number of ip addresses depends on the instance type of the node. Each instance type has a maximum number of ENIs and IP addresses per ENI. The total number of IP addresses available for pods on a node is determined by the formula:
        Total IP addresses = (Number of ENIs) * (IP addresses per ENI - 1) [ ENI > Elastic Network Interface ]
    The "-1" accounts for the primary IP address assigned to the ENI itself.
    For example, if a node is of instance type t3.medium, which supports 3 ENIs and 6 IP addresses per ENI, the total number of IP addresses available for pods would be:
        Total IP addresses = 3 * (6 - 1) = 15
    Therefore, a t3.medium node can support up to 15 pods with unique IP addresses.    


#### Network policy:
- Network Policy:
    - a Kubernetes resource that defines rules for controlling network traffic between pods and services within a cluster
    - allows administrators to specify how groups of pods are allowed to communicate with each other and with external resources
    - helps enhance security by restricting traffic flow based on defined policies
    - can be created and managed using YAML files or kubectl commands
    - supports features such as ingress and egress rules, pod selectors, and namespace selectors
    - enables fine-grained control over network communication, allowing for isolation and segmentation of workloads
    - helps prevent unauthorized access and mitigate potential security threats within the cluster
    - can be used to enforce compliance with organizational security policies and best practices
    - works in conjunction with Kubernetes networking plugins to implement the defined policies
  - Common Use Cases for Network Policies:
    - Isolating Sensitive Workloads: Restricting communication between sensitive pods and other parts of the cluster.
    - Limiting Access to Services: Controlling which pods can access specific services within the cluster.
    - Enforcing Compliance: Implementing network policies to meet regulatory and organizational security    
        requirements.
    - Segmentation of Environments: Creating separate network segments for different environments (e.g.,
        development, staging, production) within the same cluster.
    - Preventing Lateral Movement: Restricting pod-to-pod communication to minimize the risk of lateral movement in case of a security breach.
    - Controlling Egress Traffic: Defining rules for outbound traffic from pods to external resources.
    - Enhancing Security Posture: Implementing network policies as part of a broader security strategy for Kubernetes clusters.
  - Example Network Policy YAML:
      refer  networkPlolicy.yaml 

#### Pod disruption budget:
- Pod Disruption Budget (PDB):
    - a Kubernetes resource that defines the minimum number or percentage of pods that must remain available during voluntary disruptions.
    - It is minimum number of pods that should be available during maintenance activities.
    - helps manage and limit the impact of voluntary disruptions, such as node maintenance or scaling operations
    - helps ensure application availability and stability during maintenance activities, such as node upgrades or   scaling operations
    - allows administrators to specify constraints on pod disruptions, preventing excessive downtime
    - can be created and managed using YAML files or kubectl commands
    - supports features such as minAvailable and maxUnavailable settings to control pod availability
    - enables fine-grained control over pod disruptions, allowing for better management of application uptime
    - helps prevent service outages and maintain user experience during planned maintenance
    - works in conjunction with Kubernetes controllers to enforce the defined disruption budgets
    - can be used to improve the reliability and resilience of applications running in Kubernetes clusters
    - To set up a Pod Disruption Budget, you need to define the minimum number or percentage of pods that must remain available during voluntary disruptions. This is done using the `minAvailable` or `maxUnavailable` fields in the PDB specification.    
- Common Use Cases for Pod Disruption Budgets:
    - Node Upgrades: Ensuring that a minimum number of pods remain available during node maintenance or upgrades.
    - Scaling Operations: Controlling pod availability during scaling activities to prevent service disruptions.
    - Rolling Updates: Maintaining application availability during rolling updates of deployments or stateful sets.
    - Maintenance Windows: Defining disruption budgets for planned maintenance activities to minimize downtime.
    - High Availability Applications: Implementing PDBs for critical applications to ensure continuous availability.
    - Disaster Recovery: Using PDBs as part of a disaster recovery strategy to maintain application uptime.
    - Compliance Requirements: Meeting organizational or regulatory requirements for application availability.
- Example Pod Disruption Budget YAML:
    refer podDisruptionBudget.yaml

  ## > pods in same will have issues with PDB as they will be evicted together. So use anti affinity to spread the pods across nodes.
     > Pod topology constraints or anti affinity can be used to ensure that pods are distributed across different nodes or zones, reducing the risk of simultaneous disruptions.
     > refer documentation for more details.  




#### #### K8 Architecture Components:
- Kubernetes Architecture Components:
    - Master Node Components:
        - API Server: The central management component that exposes the Kubernetes API and serves as the entry  point for all administrative tasks.
        - etcd: A distributed key-value store that stores the cluster's configuration data and state.
        - Controller Manager: A component that runs various controllers to manage the state of the cluster, such as node management, replication, and endpoint management.
            - Node Controller: Responsible for monitoring the health of nodes and managing their lifecycle.
            - Replication Controller: Ensures that the desired number of pod replicas are running at all times.
            - Endpoint Controller: Manages the endpoints for services, ensuring that they are correctly mapped to the appropriate pods.
            - Cloud Controller Manager: Integrates Kubernetes with cloud provider APIs to manage resources such as load balancers, storage, and networking.
            - Cluster Controller: Manages cluster-wide resources and ensures the overall health of the cluster.
            - service Account & Token Controllers: Manage service accounts and their associated tokens for authentication and authorization.
            - job Controller: Manages the execution of batch jobs within the cluster.
            - Node Lifecycle Controller: Monitors the lifecycle of nodes and takes action based on their status.
            - Resource Quota Controller: Enforces resource quotas for namespaces to limit resource consumption.
            - DaemonSet Controller: Manages DaemonSets, ensuring that a copy of a pod runs on all or selected nodes.
            - Endpoint Slice Controller: Manages EndpointSlices, which provide a scalable way to track network endpoints for services.
            - StatefulSet Controller: Manages StatefulSets, which are used for stateful applications that require stable network identities and persistent storage.
            - Horizontal Pod Autoscaler Controller: Automatically scales the number of pod replicas based on observed CPU utilization or other metrics.
            - Backup Controller: Manages backup and restore operations for cluster resources.
            - Certificate Controller: Manages TLS certificates for secure communication within the cluster.
            - Service Controller: Manages services and their associated load balancers.
        - Scheduler: Responsible for assigning pods to nodes based on resource availability and other constraints.
    - Worker Node Components:
        - Kubelet: An agent that runs on each worker node, responsible for managing the lifecycle of pods and containers.
        - Kube-Proxy: A network proxy that runs on each worker node, responsible for maintaining network rules and facilitating communication between pods and services.
        - Container Runtime: The software responsible for running containers on the worker nodes, such as Docker, containerd, or CRI-O.
    - Add-ons:
        - DNS: A cluster-wide DNS service that provides name resolution for services and pods.
        - Dashboard: A web-based user interface for managing and monitoring the Kubernetes cluster.
        - Ingress Controller: A component that manages external access to services within the cluster, typically using HTTP/HTTPS.
        - Monitoring and Logging: Tools and services for collecting and analyzing metrics and logs from the cluster and its applications.
    - Networking:
        - CNI (Container Network Interface): A set of specifications and libraries for configuring network interfaces in Linux containers.
        - Service Mesh: An infrastructure layer that manages service-to-service communication, providing features such as load balancing, service discovery, and security.
    - Storage:
        - Persistent Volumes (PV): A cluster-wide resource that provides persistent storage for pods.   
        - Persistent Volume Claims (PVC): A request for storage by a pod, which binds to a PV.
        - Storage Classes: A way to define different types of storage with varying performance and availability characteristics.
    - Security:
        - RBAC (Role-Based Access Control): A mechanism for controlling access to cluster resources based on user roles and permissions.
        - Network Policies: Rules that define how pods can communicate with each other and with external resources.
        - Secrets: A way to store and manage sensitive information, such as passwords and API keys, within the cluster.
    - Cluster Management:
        - kubectl: The command-line tool for interacting with the Kubernetes API and managing cluster resources.
        - Helm: A package manager for Kubernetes that simplifies the deployment and management of applications and services within the cluster.
        - Operators: Custom controllers that extend Kubernetes functionality to manage complex applications and services.
             




