#### Installing kubernetes with aws eksctl
  1. Install kubectl command line tool
    For linux:
    ```bash
    curl -O https://s3.us-west-2.amazonaws.com/amazon-eks/1.34.2/2025-11-13/bin/linux/amd64/kubectl
    chmod +x ./kubectl
    mkdir -p $HOME/bin && cp ./kubectl $HOME/bin/kubectl && export PATH=$HOME/bin:$PATH 

  2. Install eksctl command line tool
    For linux:
    ```bash
    # for ARM systems, set ARCH to: `arm64`, `armv6` or `armv7`
    ARCH=amd64
    PLATFORM=$(uname -s)_$ARCH
    curl -sLO "https://github.com/eksctl-io/eksctl/releases/latest/download/eksctl_$PLATFORM.tar.gz"
    # (Optional) Verify checksum
    curl -sL "https://github.com/eksctl-io/eksctl/releases/latest/download/eksctl_checksums.txt" | grep $PLATFORM | sha256sum --check
    tar -xzf eksctl_$PLATFORM.tar.gz -C /tmp && rm eksctl_$PLATFORM.tar.gz
    sudo install -m 0755 /tmp/eksctl /usr/local/bin && rm /tmp/eksctl 

  3. Create a kubernetes cluster with eksctl
    ```bash
    eksctl create cluster --name=<cluster-name> --region=<aws-region> --nodes=<number-of-nodes> --node-type=<instance-type>
    # Example:
    eksctl create cluster --config-file=cluster-config.yaml
    eksctl delete cluster --config-file=cluster-config.yaml



#### kubernetes commands
- kubectl api-resources
- kubectl get nodes    
- kubectl get pods
- kubectl get services
- kubectl get deployments
- kubectl describe pod <pod-name>
- kubectl describe service <service-name>
- kubectl describe deployment <deployment-name>
- kubectl logs <pod-name>
- kubectl apply -f <file.yaml>
- kubectl delete -f <file.yaml>
- kubectl exec -it <pod-name> -- /bin/bash
- kubectl scale deployment <deployment-name> --replicas=<number-of-replicas>
- kubectl port-forward <pod-name> <local-port>:<pod-port>
- kubectl get namespaces
- kubectl create namespace <namespace-name>
- kubectl delete namespace <namespace-name>
- kubectl config use-context <context-name>
- kubectl get all -n <namespace-name>
- kubectl exec -it <pod-name> -c <container-name> -- <command> # Execute command in specific container within a pod
- kubectl exec -it <pod-name> -- <command> # Execute command in the first container of the pod if multiple containers exist or single container pod
- kubectl config set-context --current --namespace=roboshop-dev # Set default namespace for current context or use kubens...
- kubectx <context-name> # Switch between contexts easily using kubectx tool when multiple clusters are used
- kubens <namespace-name> # Switch between namespaces easily using kubens tool when multiple namespaces are used
- kubectl get nodes --show-labels # Show labels of nodes for getting selectors.
- kubectl taint nodes <node-name> key=value:effect # Add taint to a node so that only pods with matching tolerations can be scheduled on it, others will be repelled
- kubectl taint nodes node1 key1=value1:NoSchedule- # to remove the taint from the node
- kubectl taint nodes <node-name> key1=value1:effect- # to remove specific taint from the node




#### kubernetes resource types
- Pods
- Services
- Deployments
- ReplicaSets
- Namespaces
- ConfigMaps
- Secrets
- PersistentVolumes
- PersistentVolumeClaims
- StatefulSets
- DaemonSets
- Ingress   
- Jobs
- CronJobs




#### Kubernetes notes
- namespace:
    - used to create multiple virtual clusters within a single physical cluster
    - useful for separating environments (e.g., development, staging, production) or teams within an organization
    - resources within a namespace are isolated from those in other namespaces
    - default namespace is "default"
    - can create custom namespaces using YAML files or kubectl commands
- pod: 
    - smallest and simplest Kubernetes object
    - represents a single instance of a running process in a cluster
    - can contain one or more containers that share the same network namespace and storage
    - pods are ephemeral and can be created, destroyed, and recreated as needed
    - managed by higher-level controllers like Deployments or StatefulSets for scalability and availability   
- labels:
    - key-value pairs attached to Kubernetes objects (e.g., pods, services, deployments)
    - used for organizing, selecting, and filtering resources
    - facilitate grouping and managing resources based on specific criteria
    - can be used in conjunction with selectors to perform operations on a subset of resources
    - labels are flexible and can be added or modified at any time without affecting the underlying resources
- Annotations:
    - key-value pairs attached to Kubernetes objects
    - used to store non-identifying metadata that can be used by tools and libraries
    - typically used for configuration or operational information that is not relevant for selection or grouping
    - unlike labels, annotations are not used for selecting or filtering resources
    - can be added or modified at any time without affecting the underlying resources 
    - special characters are allowed in annotation keys and values, making them suitable for storing larger or more complex data   
## Annotations vs labels
| Feature        | Labels                                           | Annotations                                     |
|----------------|--------------------------------------------------|-------------------------------------------------|
| Purpose        | Used for identifying and grouping resources      | Used for storing non-identifying metadata       |
| Selection      | Can be used with selectors to filter resources   | Not used for selection or filtering             |
| Metadata Type  | Key-value pairs                                  | Key-value pairs                                 | 
| Use Cases      | Organizing resources, managing deployments       | Storing configuration or operational information|
| Modification   | Can be added or modified at any time             | Can be added or modified at any time            |    
| Special Chars  | Limited to alphanumeric characters, '-', '_'     | Allows special characters, larger data          |

- pod/container resources:
    - requests:
        - specify the minimum amount of CPU and memory resources required for a container to run
        - used by the Kubernetes scheduler to determine which node can accommodate the pod
        - ensures that the container has enough resources to function properly
    - limits:
        - specify the maximum amount of CPU and memory resources that a container can use
        - prevent a container from consuming excessive resources and impacting other containers on the same node
        - if a container exceeds its resource limits, it may be throttled or terminated by the kubelet
    - it is important to set appropriate resource requests and limits to ensure optimal performance and stability of applications running in Kubernetes

- Config maps:
    - used to store non-confidential configuration data in key-value pairs
    - allow separation of configuration from application code, making it easier to manage and update configurations without modifying the application
    - can be created using YAML files or kubectl commands
    - can be consumed by pods as environment variables, command-line arguments, or mounted as files in a volume
    - useful for managing application settings, feature flags, and other configuration data that may change frequently    

- Secrets:
    - used to store sensitive information such as passwords, API keys, and certificates
    - data is stored in an encoded format (base64) to provide a basic level of obfuscation
    - can be created using YAML files or kubectl commands
    - can be consumed by pods as environment variables or mounted as files in a volume
    - help enhance security by keeping sensitive data separate from application code and configuration

- Services:
They are used to achieve pod communication and load balancing in a Kubernetes cluster and expose applications running on a set of pods. Key points about services:
    - an abstraction that defines a logical set of pods and a policy to access them
    - provide stable IP addresses and DNS names for accessing pods, even as the underlying pods are created and destroyed
    - enable load balancing and service discovery within the cluster
    - can be exposed externally to allow access from outside the cluster
    - use selectors to identify the pods that belong to the service based on labels
There are four main types of services in Kubernetes:
    - ClusterIP (default): For internal communication within the cluster
        - exposes the service on a cluster-internal IP
        - accessible only within the cluster
        - used for communication between pods within the cluster
    - NodePort: For external access to the service
        - exposes the service on each node's IP at a static port (the NodePort)
        - accessible from outside the cluster using <NodeIP>:<NodePort>
        - useful for development and testing purposes
    - LoadBalancer: For external access with load balancing
        - provisions an external load balancer (if supported by the cloud provider)
        - exposes the service to the internet using a public IP address
        - routes traffic to the appropriate pods based on load balancing algorithms
    - ExternalName:
        - maps the service to an external DNS name
        - does not create a traditional service with selectors or endpoints
        - useful for integrating with external services outside the cluster    

- Replicasets:
    - ensure that a specified number of pod replicas are running at any given time
    - monitor the state of pods and create or delete pods as needed to maintain the desired number of replicas
    - provide high availability and fault tolerance for applications by distributing pod replicas across multiple nodes
    - can be created and managed using YAML files or kubectl commands
    - often used in conjunction with Deployments, which provide additional features such as rolling updates and rollbacks

- Deployment:
    - a higher-level abstraction that manages ReplicaSets and provides declarative updates to applications
    - allows you to define the desired state of your application, including the number of replicas, container images, and update strategies
    - automatically creates and manages ReplicaSets to ensure that the desired number of pod replicas are running
    - supports rolling updates, allowing you to update your application with zero downtime by gradually replacing old pods with new ones
    - provides rollback capabilities, enabling you to revert to a previous version of your application if needed
    - can be created and managed using YAML files or kubectl commands  

- Volumes:
    - provide persistent storage for pods in a Kubernetes cluster
    - allow data to persist beyond the lifecycle of individual pods, enabling stateful applications
    - can be used to share data between containers within the same pod
    - support various types of storage backends, including local storage, network-attached storage, and cloud-based storage solutions
    - can be defined in pod specifications and mounted into containers at specified paths
    - help ensure data durability and availability for applications running in Kubernetes

    - There are several types of volumes in Kubernetes, including:
        - emptyDir: A temporary directory that is created when a pod is assigned to a node and exists as long as the pod is running. It is useful for sharing data between containers in the same pod. For example , a web server container and a logging container can share log files using an emptyDir volume.
        - hostPath: Mounts a file or directory from the host node's filesystem into the pod. Useful for accessing host resources or sharing data between pods on the same node.
        - persistent Volume: Represents a piece of storage in the cluster that has been provisioned by an administrator or dynamically provisioned using StorageClasses. It is used to provide persistent storage for pods.
        - persistentVolumeClaim: Uses a PersistentVolumeClaim to request storage from a PersistentVolume and mount it into a pod. It is dynamically provisioned based on the claim's specifications.
        - configMap: Mounts a ConfigMap as a volume, allowing pods to consume configuration data
        - secret: Mounts a Secret as a volume, enabling pods to access sensitive information securely
        - nfs, awsElasticBlockStore, gcePersistentDisk, and others: Support for various external storage systems        

- Horizontal Pod Autoscaler (HPA):
    - automatically scales the number of pod replicas in a deployment or replica set based on observed CPU utilization or other select metrics
    - helps ensure that applications can handle varying levels of traffic and workload by adjusting the number of running pods as needed
    - can be configured using YAML files or kubectl commands
    - supports custom metrics in addition to CPU utilization for scaling decisions
    - improves application availability and performance by dynamically adapting to changing resource demands
    - requires the Metrics Server to be deployed in the cluster to collect resource usage data
    - can be created using the `kubectl autoscale` command or by defining an HPA resource in a YAML file
    - allows you to specify minimum and maximum replica counts, as well as target CPU utilization or custom metrics for scaling decisions
    - helps optimize resource usage and costs by scaling down when demand is low and scaling up when demand increases
    - requires proper resource requests and limits to be set on the pods for effective scaling

 - Vertical Pod Autoscaler (VPA):
    - automatically adjusts the CPU and memory resource requests and limits for containers in pods based on observed usage
    - helps ensure that applications have the appropriate resources to operate efficiently without over-provisioning or under-provisioning
    - can be configured using YAML files or kubectl commands
    - supports three modes: "Off" (no adjustments), "Auto" (automatic adjustments), and "Recreate" (restarts pods to apply new resource settings)
    - improves application performance and stability by adapting to changing resource demands over time
    - can be used in conjunction with Horizontal Pod Autoscaler (HPA) for comprehensive scaling strategies
    - requires the VPA admission controller to be enabled in the cluster for automatic resource adjustments
    - helps optimize resource usage and costs by ensuring that pods have the right amount of resources based on actual usage patterns   



   
    


#### Errors
- CrashLoopBackOff:
    - indicates that a container in a pod is repeatedly crashing and restarting
    - common causes include application errors, misconfigurations, or resource constraints
    - to troubleshoot, check the container logs using `kubectl logs <pod-name>` and inspect the pod description with `kubectl describe pod <pod-name>`
    - ensure that the container image is correct and that all required dependencies are available
    - consider adding readiness and liveness probes to monitor the health of the application within the container
- ImagePullBackOff:
    - indicates that Kubernetes is unable to pull the specified container image from the container registry
    - common causes include incorrect image names, missing tags, or authentication issues with private registries
    - to troubleshoot, check the pod description with `kubectl describe pod <pod-name>` to see detailed error messages related to image pulling
    - verify that the image name and tag are correct and that the container registry is accessible
    - if using a private registry, ensure that the necessary image pull secrets are configured in the pod specification



## Useful stuff

- liveness and readiness probes:
    - used to monitor the health and availability of containers within a pod
    - liveness probes determine if a container is running and healthy; if the probe fails, the container is restarted
    - readiness probes determine if a container is ready to accept traffic; if the probe fails, the pod is removed from service endpoints
    - can be configured using various methods such as HTTP requests, TCP socket checks, or command execution
    - help improve application reliability and availability by ensuring that only healthy containers receive traffic and that unhealthy containers are automatically recovered
liveness prove vs readiness probe:
| Feature            | Liveness Probe                               | Readiness Probe                                     |
|--------------------|----------------------------------------------|-----------------------------------------------------|
| Purpose            | Checks if the container is alive and healthy | Checks if the container is ready to serve traffic.  |
| Action on Failure  | Restarts the container                       | Removes the pod from service endpoints              |
| Use Case           | Detects and recovers from application crashes| Manages traffic routing based on container readiness|
| Configuration      | Can use HTTP, TCP, or command execution      | Can use HTTP, TCP, or command execution             |
| Impact on Traffic  | No direct impact on traffic routing          | Directly affects traffic routing to the pod         |




- kubectx and kubens:
    - kubectx is a command-line tool that simplifies switching between multiple Kubernetes contexts (clusters)
    - kubens is a command-line tool that simplifies switching between multiple Kubernetes namespaces within a cluster
    - both tools enhance productivity by allowing users to quickly change their working context or namespace without manually editing the kubeconfig file
    - can be installed using package managers like Homebrew or by downloading precompiled binaries from their respective GitHub repositories
    - once installed, users can switch contexts with `kubectx <context-name>` and switch namespaces with `kubens <namespace-name>`


- configmap vs secret:
| Feature        | ConfigMap                                      | Secret                                           |
|----------------|------------------------------------------------|-------------------------------------------------|
| Purpose        | Stores non-confidential configuration data     | Stores sensitive information                     |
| Data Encoding  | Plain text                                     | Base64 encoded                                  |
| Use Cases      | Application settings, feature flags            | Passwords, API keys, certificates                  |
| Access Methods | Environment variables, volume mounts           | Environment variables, volume mounts             |
| Security       | Not encrypted, suitable for non-sensitive data | Provides basic obfuscation, suitable for sensitive data |   

when config mapchanged, pods or deployments using it need to be restarted to pick up the changes. Secrets behave similarly. 


- EBS 
    - To mount an EBS volume to a pod, you need to create a PersistentVolume (PV) and a PersistentVolumeClaim (PVC) that references the EBS volume.
      Then you can mount the PVC to the pod in the pod specification.
    - EBS volumes are specific to a single availability zone, so ensure that your pods are scheduled in the same zone as the EBS volume.
    - EBS volumes can be dynamically provisioned using StorageClasses, allowing for automatic creation and management of EBS volumes based on PVC requests.
    - Install the AWS EBS CSI driver in your Kubernetes cluster to enable dynamic provisioning and management of EBS volumes.
        kubectl apply -k "github.com/kubernetes-sigs/aws-ebs-csi-driver/deploy/kubernetes/overlays/stable/?ref=release-1.53"
    - EC2 instances (nodes) must have the necessary IAM permissions to access and manage EBS volumes.
    - Create PV 

- EFS
    - To mount an EFS volume to a pod, you need to create a PersistentVolume (PV) and a PersistentVolumeClaim (PVC) that references the EFS file system.
      Then you can mount the PVC to the pod in the pod specification.
    - EFS volumes are accessible across multiple availability zones, making them suitable for applications that require shared storage.
    - Install the AWS EFS CSI driver in your Kubernetes cluster to enable dynamic provisioning and management of EFS volumes.
        kubectl kustomize \
    "github.com/kubernetes-sigs/aws-efs-csi-driver/deploy/kubernetes/overlays/stable/?ref=release-2.2" > public-ecr-driver.yaml
    - EC2 instances (nodes) must have the necessary IAM permissions to access and manage EFS file systems.
    - Create PV and PVC for EFS and mount it to pods as needed.
    - Create a security group that allows NFS traffic (port 2049) between the EFS file system and the EC2 instances (nodes) in your Kubernetes cluster.

- EBS vs EFS:
| Feature               | EBS (Elastic Block Store)                     | EFS (Elastic File System)                     |
|-----------------------|-----------------------------------------------|-------------------------------------------------|
| Storage Type          | Block storage                                 | File storage                                   |
| Accessibility         | Accessible by a single EC2 instance at a time | Accessible by multiple EC2 instances simultaneously |
| Use Cases             | Databases, file systems, applications requiring low-latency access | Shared file storage, content management systems, home directories |
| Performance           | High performance for single-instance workloads | Scalable performance for multi-instance workloads |
| Durability            | Data is replicated within a single availability zone | Data is replicated across multiple availability zones |
| Cost                  | Generally lower cost for single-instance use cases | Generally higher cost due to multi-instance access and scalability | 
| Mounting in Pods     | Requires PV and PVC for mounting               | Requires PV and PVC for mounting                 |
| Dynamic Provisioning | Supported with AWS EBS CSI driver                | Supported with AWS EFS CSI driver                |
| IAM Permissions      | Required for EC2 instances to manage EBS volumes | Required for EC2 instances to manage EFS file systems |
| Performance Modes     | General Purpose (gp2, gp3), Provisioned IOPS (io1, io2) | Bursting Throughput, Provisioned Throughput      |
| Scalability          | Up to 16 TiB per volume                        | Scales up to petabytes automatically            |
| Backup and Snapshots | Supports snapshots for data backup and recovery | Supports backups through AWS Backup service      |
| Data Sharing        | Not designed for data sharing between instances | Designed for concurrent access by multiple instances |
| Integration with AWS Services | Integrates with EC2, RDS, and other services | Integrates with EC2, ECS, EKS, and other services |
| Performance Consistency | Provides consistent performance for single-instance workloads | Performance may vary based on the number of concurrent connections and workload |
| Security            | Supports encryption at rest and in transit     | Supports encryption at rest and in transit       |
| Security groups      | EBS volumes inherit the security groups of the EC2 instance | EFS file systems can be associated with specific security groups |        


#### sets 

- Replica set:
    - ensures that a specified number of pod replicas are running at any given time
    - monitors the state of pods and creates or deletes pods as needed to maintain the desired number of replicas
    - provides high availability and fault tolerance for applications by distributing pod replicas across multiple nodes
    - can be created and managed using YAML files or kubectl commands
    - often used in conjunction with Deployments, which provide additional features such as rolling updates and rollbacks

- Deployment:
    - a higher-level abstraction that manages ReplicaSets and provides declarative updates to applications
    - allows you to define the desired state of your application, including the number of replicas, container images, and update strategies
    - automatically creates and manages ReplicaSets to ensure that the desired number of pod replicas are running
    - supports rolling updates, allowing you to update your application with zero downtime by gradually replacing old pods with new ones
    - provides rollback capabilities, enabling you to revert to a previous version of your application if needed
    - can be created and managed using YAML files or kubectl commands
    - It is for stateless applications where any pod can handle any request

- DaemonSets:
    - ensure that a copy of a specific pod is running on all (or a subset of) nodes in a Kubernetes cluster
    - commonly used for deploying system-level services such as log collectors, monitoring agents, or network plugins or logging agents.
    - automatically adds pods to new nodes as they are added to the cluster and removes pods from nodes that are removed
    - can be created and managed using YAML files or kubectl commands
    - provide a way to ensure that essential services are consistently available across all nodes in the cluster    
    - logs are accessed by hostPath volume or DaemonSet pods can forward logs to a centralized logging system

 - StatefulSets:
    - manage the deployment and scaling of stateful applications in Kubernetes
    - provide guarantees about the ordering and uniqueness of pod instances
    - maintain a stable network identity and persistent storage for each pod, even when pods are rescheduled or restarted
    - commonly used for applications that require stable identities and persistent data, such as databases or distributed systems
    - can be created and managed using YAML files or kubectl commands   
    - It is for stateful applications where each pod has its own identity and storage
    - A Headless service is mandatory to be created for a statefulset to function properly.

- Headless Services:
    - a type of Kubernetes service that does not allocate a cluster IP address
    - used to expose pods directly without load balancing or proxying
    - commonly used in conjunction with StatefulSets to provide stable network identities for each pod
    - allows clients to connect directly to individual pod instances using their DNS names
    - can be created and managed using YAML files or kubectl commands   
    - useful for stateful applications that require direct access to specific pod instances, such as databases or clustered applications
    - When a headless service is created, Kubernetes does not assign it a cluster IP address. Instead, it creates DNS records for each pod that matches the service's selector. This allows clients to resolve the DNS name of the headless service to the individual pod IP addresses, enabling direct communication with specific pod instances.
    - when a headless sdervie is hit, it will give all the pod IPs. It is for data repplication and clustering of stateful applications(like DBs  etc).
    - It is mandatory to create a headless service for statefulsets. 


Deployment vs Statefulset vs Daemonset:
| Feature               | Deployment                                  | StatefulSet                                   | DaemonSet                                    |
|-----------------------|---------------------------------------------|-----------------------------------------------|----------------------------------------------|
| Purpose               | Manages stateless applications               | Manages stateful applications                  | Ensures a pod runs on all or specific nodes |
| Pod Identity          | Pods are interchangeable                     | Each pod has a unique identity and stable network ID | Pods run on all or specific nodes            |
| Storage               | Typically uses shared storage or no storage  | Provides stable, persistent storage for each pod | Can use hostPath or other storage types       |
| Scaling               | Easy to scale up or down                      | Scaling maintains pod identity and order       | Not typically scaled; runs on all nodes         |
| Update Strategy       | Supports rolling updates and rollbacks       | Supports rolling updates with ordered pod termination | Updates can be applied to all pods simultaneously |
| Use Cases             | Web servers, APIs, front-end applications | Databases, distributed systems, stateful apps | Log collectors, monitoring agents, network plugins |
| Network Identity      | Pods share the same network identity         | Each pod has a unique network identity         | Pods share the same network identity          |
| Pod Scheduling      | Pods can be scheduled on any node            | Pods are scheduled based on their identity and storage | Pods are scheduled on all or specific nodes   |    


- Volumes:
    - provide persistent storage for pods in a Kubernetes cluster
    - allow data to persist beyond the lifecycle of individual pods, enabling stateful applications
    - can be used to share data between containers within the same pod
    - support various types of storage backends, including local storage, network-attached storage, and cloud-based storage solutions
    - can be defined in pod specifications and mounted into containers at specified paths
    - help ensure data durability and availability for applications running in Kubernetes
  - Static provisioning:
      - An administrator manually creates PersistentVolumes (PVs) in the cluster.
      - Users create PersistentVolumeClaims (PVCs) to request storage from the available PVs.
      - The PVC is bound to a matching PV based on size and access modes.
  - Dynamic provisioning:
      - Storage class is created by the administrator to define different types of storage (e.g., SSD, HDD) and their parameters.
      - Users create PVCs without pre-existing PVs.
      - The cluster automatically provisions a new PV based on the PVC's specifications using StorageClasses. 


 Scheduling in Kubernetes:
  - Kubernetes uses a scheduler to assign pods to nodes based on resource availability and other constraints.
  - The scheduler considers factors such as CPU and memory requests, node affinity/anti-affinity, taints and tolerations, and custom scheduling policies.
  - The default scheduler is kube-scheduler, but custom schedulers can also be implemented for specific use cases.
  - Scheduling decisions are made based on a two-step process: filtering and scoring.
    - Filtering: The scheduler filters out nodes that do not meet the pod's requirements (e.g., insufficient resources, taints).
    - Scoring: The scheduler scores the remaining nodes based on various criteria (e.g., resource utilization, affinity rules) and selects the best-fit node for the pod.
  - Users can influence scheduling decisions by specifying resource requests and limits, node selectors, affinity rules, and tolerations in the pod specification.  
- Taints and Tolerations:
    - Taints are applied to nodes to repel certain pods from being scheduled on them.
    - Tolerations are applied to pods to allow them to be scheduled on nodes with matching taints.
    - Taints and tolerations work together to control pod placement and ensure that specific workloads are isolated or prioritized on certain nodes.
    - Common use cases include dedicating nodes for specific workloads, isolating critical applications, or preventing certain pods from running on specific nodes.
    - Taints can be added to nodes using kubectl commands or defined in node specifications, while tolerations are specified in pod specifications.

    - Adding tolerations to pods allows them to be scheduled on nodes with matching taints, but its not guraranteed to be scheduled only in the tainted node(so use nodeselector), while nodes without the appropriate tolerations will repel those pods.
    - NoSchedule effect: Pods that do not tolerate the taint will not be scheduled on the node.
    - PreferNoSchedule effect: The scheduler will try to avoid placing pods that do not tolerate the taint on the node, but it is not guaranteed.
    - NoExecute effect: Pods that do not tolerate the taint will be evicted from the node if they are already running there. If a pod tolerates the taint, it can continue to run on the node till tolerationSeconds expires, if no tolerationSeconds is set, it can run indefinitely.
      

    taints vs tolerations:
| Feature        | Taints                                         | Tolerations                                     |
|----------------|------------------------------------------------|-------------------------------------------------|
| Purpose        | Repel pods from being scheduled on nodes       | Allow pods to be scheduled on nodes with matching taints |
| Applied To    | Nodes                                          | Pods                                            |
| Use Cases      | Isolating workloads, dedicating nodes          | Allowing specific pods to run on tainted nodes |
| Configuration  | Added to nodes using kubectl or node specs      | Specified in pod specifications                 |
| Interaction     | Work together with tolerations to control pod placement | Work together with taints to control pod placement |
| Example        | Taint a node to repel all pods except those with matching tolerations | Tolerate a specific taint to allow scheduling on tainted nodes |
| Effect on Scheduling | Prevents pods without matching tolerations from being scheduled on the node | Allows pods with matching tolerations to be scheduled on the node |
| Impact on Workloads | Helps ensure that specific workloads are isolated or prioritized on certain nodes | Helps ensure that specific workloads can run on nodes with specific taints |
| Configuration Syntax | `kubectl taint nodes <node-name> key=value:effect` | Specified in pod YAML under `tolerations` field |
| Examples       | `kubectl taint nodes node1 dedicated=group1:NoSchedule` | `tolerations: - key: "dedicated" operator: "Equal" value: "group1" effect: "NoSchedule"` |    



- Affinity and Anti-Affinity:
    - Affinity rules specify preferences for scheduling pods on nodes based on labels, allowing pods to be co-located on the same node or in the same zone.
    - Anti-affinity rules specify preferences for avoiding scheduling pods on nodes with certain labels, helping to distribute workloads across nodes or zones.
    - Both affinity and anti-affinity can be defined at the pod level using node affinity (for node selection) or pod affinity/anti-affinity (for co-locating or separating pods).
    - Affinity and anti-affinity rules can be specified as required (hard constraints) or preferred (soft constraints), influencing the scheduler's decisions without being mandatory.
    - Common use cases include improving application performance by co-locating related pods, enhancing fault tolerance by distributing pods across failure domains, and optimizing resource utilization by balancing workloads across nodes.
    - Affinity and anti-affinity rules are defined in the pod specification under the `affinity` field.
    - Node Affinity: Used to constrain which nodes a pod can be scheduled on based on node labels.
    - Pod Affinity: Used to co-locate pods on the same node or in the same zone based on pod labels.
    - Pod Anti-Affinity: Used to avoid co-locating pods on the same node or in the same zone based on pod labels.
| Feature        | Affinity                                      | Anti-Affinity                                  |
|----------------|-----------------------------------------------|------------------------------------------------|
| Purpose        | Specifies preferences for scheduling pods on nodes based on labels | Specifies preferences for avoiding scheduling pods on nodes with certain labels |
| Types          | Node Affinity, Pod Affinity                     | Pod Anti-Affinity                              |
| Use Cases      | Co-locating related pods, improving performance | Distributing workloads, enhancing fault tolerance |
| Configuration  | Defined in pod specification under `affinity` field | Defined in pod specification under `affinity` field |
| Interaction     | Works together with anti-affinity to control pod placement | Works together with affinity to control pod placement |
| Effect on Scheduling | Influences scheduler to place pods on preferred nodes or with preferred pods | Influences scheduler to avoid placing pods on certain nodes or with certain pods |
| Impact on Workloads | Helps optimize resource utilization and application performance | Helps ensure workload distribution and fault tolerance |
| Configuration Syntax | Specified in pod YAML under `affinity` field | Specified in pod YAML under `affinity` field |
| Examples       | Node Affinity: `requiredDuringSchedulingIgnoredDuringExecution` | Pod Anti-Affinity: `preferredDuringSchedulingIgnoredDuringExecution` |
| Pod Affinity: |`requiredDuringSchedulingIgnoredDuringExecution` |
| Pod Anti-Affinity: | `requiredDuringSchedulingIgnoredDuringExecution` |   

     - node affinity:
        - Its an enhanced version of nodeSelector.
        - allows more expressive rules for pod scheduling based on node labels.
        - has more operators like In, NotIn, Exists, DoesNotExist, Gt, Lt.
        - requiredDuringSchedulingIgnoredDuringExecution: Pods must be scheduled on nodes that match the specified label criteria. hard requirement. pod status pending if not matched.
        - preferredDuringSchedulingIgnoredDuringExecution: Pods prefer to be scheduled on nodes that match the specified label criteria, but it is not mandatory. soft requirement. pod can be scheduled on non-matching nodes if necessary.
           - multiple preferences can be specified with different weights to influence scheduling decisions. 
           - the scheduler will try to place the pod on nodes that satisfy the highest-weighted preferences first.
        - Not<operator>: Used to specify that a node should not have certain labels for the pod to be scheduled there. It is anti-affinity at node level.
    
    - pod affinity:
        - allows pods to be co-located on the same node or in the same zone based on pod labels.
        - has In, NotIn, Exists, DoesNotExist as operators.
        - used to improve application performance by placing related pods together.
        - requiredDuringSchedulingIgnoredDuringExecution: Pods must be scheduled on nodes that already have pods with the specified labels. hard requirement.
        - preferredDuringSchedulingIgnoredDuringExecution: Pods prefer to be scheduled on nodes that already have pods with the specified labels, but it is not mandatory. soft requirement.
           - multiple preferences can be specified with different weights to influence scheduling decisions.
           - the scheduler will try to place the pod on nodes that satisfy the highest-weighted preferences first.
        - Not<operator>: Used to specify that a pod should not be co-located with certain pods for the pod to be scheduled there. It is anti-affinity at pod level.   





#### Helm charts.
- Helm:
    - a package manager for Kubernetes that simplifies the deployment and management of applications
    - uses charts, which are pre-configured packages of Kubernetes resources, to define, install, and upgrade applications
    - allows users to easily share and distribute applications through Helm repositories
    - provides features such as templating, versioning, and dependency management for Kubernetes applications
    - can be used to deploy complex applications with multiple components and configurations using a single command
    - helps streamline the application lifecycle management process in Kubernetes environments
- Helm Charts:
    - a collection of files that describe a related set of Kubernetes resources
    - typically include templates for Kubernetes manifests, configuration values, and metadata about the chart
    - can be customized using values files or command-line arguments during installation or upgrade
    - can be stored in Helm repositories for easy sharing and distribution
    - allow for easy installation, upgrade, and rollback of applications in Kubernetes clusters
    - can define dependencies on other charts, enabling the deployment of complex applications with multiple components       

    - Installing Helm:
        - Download the Helm binary from the official Helm website or GitHub repository.
        - Extract the binary and move it to a directory in your system's PATH (e.g., /usr/local/bin).
        - Verify the installation by running `helm version` in your terminal.
            # curl -fsSL -o get_helm.sh https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-4
            # chmod 700 get_helm.sh
            # ./get_helm.sh
    - Adding Helm Repositories:
        - Use the `helm repo add` command to add a Helm repository. For example:
          `helm repo add stable https://charts.helm.sh/stable`
        - Update the local Helm repository cache using `helm repo update`.
    - Installing Helm Charts:
        - Use the `helm install` command to install a Helm chart. For example:
          `helm install my-release stable/nginx`
        - You can customize the installation by providing a values file using the `-f` flag or by setting individual values using the `--set` flag.
    - Managing Helm Releases:
        - Use the `helm list` command to view installed Helm releases.
        - Upgrade a release using the `helm upgrade` command. For example:
          `helm upgrade my-release stable/nginx`
        - Rollback a release to a previous version using the `helm rollback` command. For example:
          `helm rollback my-release 1`  
        - Uninstall a release using the `helm uninstall` command. For example:
          `helm uninstall my-release`
        - View the status of a release using the `helm status` command. For example:
          `helm status my-release`

    - Hemlo commands:
        - `helm create <chart-name>`: Creates a new Helm chart with the specified name.
        - `helm lint <chart-path>`: Validates a Helm chart for correctness and best practices.
        - `helm package <chart-path>`: Packages a Helm chart into a .tgz file for distribution.
        - `helm search repo <keyword>`: Searches for charts in the configured Helm repositories.
        - `helm get all <release-name>`: Retrieves all information about a specific Helm release.
        - `helm history <release-name>`: Displays the revision history of a specific Helm release.
        - `helm install <chart-name>  Installs a Helm chart with a generated release name.      
        - `helm install <chart-name> .  Installs a Helm chart 
        - `helm install <release-name> <chart-name>`: Installs a Helm chart with a specified release name.
        - `helm upgrade <release-name> <chart-name>`: Upgrades an existing Helm
        - helm rollback <release-name> <revision-number>: Rolls back a Helm release to a specific revision.
        - helm list: Lists all installed Helm releases in the current namespace.
        - helm uninstall <release-name>: Uninstalls a Helm release from the cluster.
        - helm repo add <repo-name> <repo-url>: Adds a Helm repository to the local configuration.
        - helm repo update: Updates the local Helm repository cache with the latest charts from the configured
        - helm search repo <keyword>: Searches for Helm charts in the configured repositories using the specified keyword.
        - helm show values <chart-name>: Displays the default values for a specified Helm chart.
        - helm template <chart-name>: Renders the templates of a Helm chart locally without installing it.
        - helm lint <chart-path>: Validates a Helm chart for correctness and best practices.    


